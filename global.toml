[llm]
# Model string format: provider:model:version[:thinking]
# See docs/reference/CONFIGURATION.md for format details and LLM_CHAT_INTERFACE.md for LLM integration
model = "anthropic:claude-sonnet-4:20250514"
# model = "anthropic:claude-opus-4:latest:thinking"  # Latest Opus 4.1 in thinking mode
# model = "openai:gpt-5-high:latest:thinking"        # GPT-5-high with thinking mode (when available)
temperature = 0.0
maxTokens = 2000
targetFunctionCount = 10

[viewer]
host = "127.0.0.1"
port = 8000
# how much the mouse wheel zooms
wheelSensitivity = 0.05

[viewer.highlight]
# Number of hops to highlight (1 = neighbors only, 2 = include neighbors-of-neighbors)
steps = 2
# Hide edges that are not part of the highlighted sets
hideNonHighlightedEdges = true

[viewer.highlight.colors]
focus = "#0ea5e9"
incoming = "#ef4444"
outgoing = "#10b981"
moduleOutline = "#0ea5e9"

[viewer.highlight.opacity]
fadedNodes = 0.15
fadedText = 0.4
secondDegreeNodes = 0.45
secondDegreeEdges = 0.35

[viewer.highlight.widths]
edge = 2
edgeHighlighted = 4
nodeBorder = 1
nodeBorderHighlighted = 3

[tags]
# Global extraction-time tag vocabulary (starter set; can be refined per project)
global = [
  "util", "config", "testing",
  "backend", "frontend", "risky-or-sensitive",
  "entrypoint", "experimental", "deprecated", "important"
]

[extractAll]
languages = ["python", "typescript", "javascript"]
